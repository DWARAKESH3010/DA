{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31211833-0016-4186-9c89-ca9c49fe4267",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d91d2965-9ffe-461b-8530-99a50ba86e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.1 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/44.1 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.1/44.1 kB 310.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB 8.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/10.1 MB 10.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/10.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.3/10.1 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.0/10.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.5/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.8/10.1 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.1/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.7/10.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.2/10.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.9/10.1 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.5/10.1 MB 11.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.2/10.1 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.3/10.1 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.1/10.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.5 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/450.5 kB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 450.5/450.5 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.3/286.3 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.6/2.4 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.4 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 12.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.0 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d245bfe1-2688-4f06-a871-a07058eefd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.6.0\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aab7b8ac-16ec-4188-9e9c-01e073769568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB 330.3 kB/s eta 0:00:06\n",
      "    --------------------------------------- 0.0/1.7 MB 495.5 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.6/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.7 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ac055-2e07-4e8d-ad4d-e08604c01010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ac4381c-5866-43fe-9958-3be1db22e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "                                                    para  \n",
       "0      I am coming to the borders and I will kill you...  \n",
       "1      im getting on borderlands and i will kill you ...  \n",
       "2      im coming on borderlands and i will murder you...  \n",
       "3      im getting on borderlands 2 and i will murder ...  \n",
       "4      im getting into borderlands and i can murder y...  \n",
       "...                                                  ...  \n",
       "74676  Just realized that the Windows partition of my...  \n",
       "74677  Just realized that my Mac window partition is ...  \n",
       "74678  Just realized the windows partition of my Mac ...  \n",
       "74679  Just realized between the windows partition of...  \n",
       "74680  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_training.csv\")\n",
    "df[\"para\"] = df[\"im getting on borderlands and i will murder you all ,\"]\n",
    "df.drop(columns=[\"im getting on borderlands and i will murder you all ,\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4436e83-2f45-4e55-b3b4-d08a75f32b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "i\n",
      "am\n",
      "under\n",
      "the\n",
      "water\n",
      "please\n",
      "help\n",
      "me\n",
      ".\n",
      "there\n",
      "two\n",
      "much\n",
      "raining\n",
      "uhhhhh.take\n",
      "this\n",
      "2\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "val = nlp(\"hello i am under the water please help me. there two much raining uhhhhh.take this 2$\")\n",
    "for i in val:\n",
    "    print(i)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3ad6390-cf8a-47f3-a66e-e83a0ce0d9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = val[-1]\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8bdde37-5a78-4385-9edf-b967873dd6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08ed6130-69ff-4233-90d4-022417323954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ff9bf6ee-eb1c-468a-997a-598624e3fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am coming to the borders.\n",
      "I will kill you.\n",
      "It is a beautiful day.\n",
      "We are going to the market.\n",
      "This is the last sentence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ' '.join(df[\"para\"].tail())\n",
    "count = 0\n",
    "doc =  nlp(o)\n",
    "for i in doc.sents:\n",
    "    print(i.text)\n",
    "    count = count + 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e09177d-b4ac-405e-9ae1-27ac53d814f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will kill you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is a beautiful day.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are going to the market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the last sentence.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          para\n",
       "0  I am coming to the borders.\n",
       "1             I will kill you.\n",
       "2       It is a beautiful day.\n",
       "3  We are going to the market.\n",
       "4   This is the last sentence."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79bd20-c43e-4094-8ef0-1d65bf945ea6",
   "metadata": {},
   "source": [
    "# pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e760346f-bdeb-4de8-adbd-720741908898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "985d3f08-4d7a-4e2a-951f-e74ceb5e9dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x235915c35f0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x235915c1d30>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x23590274e40>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x23591d10550>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x23591d14b10>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x23590274cf0>)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5170777-b726-40c0-bae6-23ad1241774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  |  INTJ  |  hello  | \n",
      "i  |  PRON  |  I  | \n",
      "am  |  AUX  |  be  | \n",
      "under  |  ADP  |  under  | \n",
      "the  |  DET  |  the  | \n",
      "water  |  NOUN  |  water  | \n",
      "please  |  INTJ  |  please  | \n",
      "help  |  VERB  |  help  | \n",
      "me  |  PRON  |  I  | \n",
      ".  |  PUNCT  |  .  | \n",
      "there  |  ADV  |  there  | \n",
      "two  |  NUM  |  two  | \n",
      "much  |  ADJ  |  much  | \n",
      "raining  |  VERB  |  rain  | \n",
      "uhhhhh  |  ADJ  |  uhhhhh  | \n",
      ".  |  PUNCT  |  .  | \n",
      "take  |  VERB  |  take  | \n",
      "this  |  PRON  |  this  | \n",
      "2  |  NUM  |  2  | \n",
      "$  |  SYM  |  $  | \n"
     ]
    }
   ],
   "source": [
    "val = nlp(\"hello i am under the water please help me. there two much raining uhhhhh. take this 2$\")\n",
    "for i in val:\n",
    "    print(i,\" | \",i.pos_,\" | \",i.lemma_,\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b389fdfe-606a-4999-90e9-0177d5146580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two  |  CARDINAL  |  Numerals that do not fall under another type\n",
      "2$  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for i in val.ents:\n",
    "    print(i,\" | \",i.label_,\" | \",spacy.explain(i.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a269be3-af03-4837-963f-1dc169ee9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc1c15462f840f7a45b7e8279dd503e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f5c4f8a27d4c28b831e77cf94c7537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647dfe2bf45349f6879d5f8c2f00ac79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          text sentiment_label  \\\n",
      "0         I love this product! It works great.        POSITIVE   \n",
      "1  This is the worst thing I have ever bought.        NEGATIVE   \n",
      "2              I am so happy with my purchase!        POSITIVE   \n",
      "3                 I will never buy this again.        NEGATIVE   \n",
      "4       The service was okay, nothing special.        NEGATIVE   \n",
      "5   Absolutely fantastic! Highly recommend it!        POSITIVE   \n",
      "\n",
      "   sentiment_score  \n",
      "0         0.999879  \n",
      "1         0.999784  \n",
      "2         0.999879  \n",
      "3         0.970337  \n",
      "4         0.986240  \n",
      "5         0.999883  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import Language\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the sentiment analysis pipeline from Hugging Face\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Custom component for sentiment analysis\n",
    "@Language.component(\"sentiment_component\")\n",
    "def sentiment_component(doc):\n",
    "    # Get sentiment prediction for the entire document\n",
    "    result = sentiment_pipeline(doc.text)\n",
    "    # Assign sentiment to the document's extension\n",
    "    doc._.sentiment = result[0]\n",
    "    return doc\n",
    "\n",
    "# Register the custom component with spaCy\n",
    "spacy.tokens.Doc.set_extension(\"sentiment\", default=None)\n",
    "nlp.add_pipe(\"sentiment_component\", last=True)\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this product! It works great.',\n",
    "        'This is the worst thing I have ever bought.',\n",
    "        'I am so happy with my purchase!',\n",
    "        'I will never buy this again.',\n",
    "        'The service was okay, nothing special.',\n",
    "        'Absolutely fantastic! Highly recommend it!'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to analyze sentiment for each text\n",
    "def analyze_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    return doc._.sentiment['label'], doc._.sentiment['score']\n",
    "\n",
    "# Apply the sentiment analysis function to the DataFrame\n",
    "df['sentiment_label'], df['sentiment_score'] = zip(*df['text'].apply(analyze_sentiment))\n",
    "\n",
    "# Display the results\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ec719-6739-4846-8302-1599c1ead8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eedae-1449-4b0a-bdcf-0eea601e1ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9c1ca-6cbe-4377-b531-09a77824efcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4bd62-56be-4b65-b7d1-88a91ad9bbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
